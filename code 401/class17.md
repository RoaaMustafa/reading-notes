# Web Scraping

## [What is Web Scraping?](https://en.wikipedia.org/wiki/Web_scraping)

Web scraping typically extracts large amounts of data from websites for a variety of uses such as price monitoring, enriching machine learning models, financial data aggregation, monitoring consumer sentiment, news tracking, etc. Browsers show data from a website. However, manually copy data from multiple sources for retrieval in a central place can be very tedious and time-consuming. Web scraping tools essentially automate this manual process.

#### What is Web Scraping or Data Mining?
Basics of Web Scraping
“Web scraping,” also called crawling or spidering, is the automated gathering of data from an online source usually from a website. While scraping is a great way to get massive amounts of data in relatively short timeframes, it does add stress to the server where the source hosted.


Web scraping typically extracts large amounts of data from websites for a variety of uses such as price monitoring, enriching machine learning models, financial data aggregation, monitoring consumer sentiment, news tracking, etc. Browsers show data from a website.

![web scraping](https://www.hirinfotech.com/wp-content/uploads/2019/10/What-is-Web-Scraping-1024x512.png)

#### Typical applications of web scraping
1- Social media sentiment analysis
2- E-Commerce pricing
3- Investment opportunities
4- Machine learning

### The 10 Best Web Scraping Tools


[Scraper API](https://www.scraperapi.com/)

[ScrapeSimple](https://www.scrapesimple.com/)

[Octoparse](https://www.octoparse.com/)

[ParseHub](https://www.parsehub.com/)

[Scrapy](https://scrapy.org)

[Diffbot](https://www.diffbot.com)

[Cheerio](https://cheerio.js.org)

[Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/)

[Puppeteer](https://github.com/GoogleChrome/puppeteer)

[Mozenda](https://www.mozenda.com/)


## [How to scrape websites without getting blocked](https://www.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/)

#### Respect Robots.txt
Web spiders should ideally follow the robot.txt file for a website while scraping. It has specific rules for good behavior such as how frequently you can scrape, which pages allow scraping, and which ones you can’t. Some websites allow Google to scrape their websites, by not allowing any other websites to scrape.


## [Web Scrape with Python in 4 minutes](https://towardsdatascience.com/how-to-web-scrape-with-python-in-4-minutes-bc49186a8460)
